{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook contains all the long-to-run code for the project. The grid search for the best parameters for the models, some graphs that are long to process, like PCA comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code isn't runned in this notebook, please refer to the other notebook for the results, if you want to run the code, please copy it over to the other notebook, at the place you want to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that RandomForest and XGBClassifier have the best performance among the models we have tried.\n",
    "# Let's try to improve the performance of these models with hyperparameter optimization.\n",
    "# We will use GridSearchCV for hyperparameter optimization.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_params = {'n_estimators':[100,200,500,1000],\n",
    "             'max_features':[3, 'sqrt', 'log2', None],\n",
    "             'min_samples_split':[2,5,10,20],\n",
    "             'max_samples': [0.25, 0.5, 0.75],\n",
    "             'random_state':[42],\n",
    "             'criterion':['gini','entropy', 'log_loss']}\n",
    "rf = RandomForestClassifier()\n",
    "rf_cv_model = GridSearchCV(rf, rf_params, cv=3, n_jobs=-1, verbose=2).fit(train_pca,y_train)\n",
    "display(rf_cv_model.best_params_)\n",
    "\n",
    "# We can see that the performance of the RandomForest model has improved, but not by a lot (+0,002%), hyperparamteter tuning is secondary as it is very time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for the best number of components for PCA\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "for i in range(1, 50):\n",
    "    pca = PCA(n_components=i,random_state=0)\n",
    "    train_pca = pca.fit_transform(scaled_train_X)\n",
    "    test_pca = pca.transform(scaled_test_X)\n",
    "    train_and_evaluate_model(XGBClassifier(random_state=0), name='XGBClassifier, n_components = {}'.format(i))\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(results['Models'],results['Accuracy'],label='Accuracy')\n",
    "plt.plot(results['Models'],results['Precision'],label='Precision')\n",
    "plt.plot(results['Models'],results['Recall'],label='Recall')\n",
    "plt.plot(results['Models'],results['F1 Score'],label='F1 Score')\n",
    "plt.plot(results['Models'],results['ROC AUC Score'],label='ROC AUC Score')\n",
    "plt.title('Model Performance')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to improve the performance of the XGBClassifier model with hyperparameter optimization.\n",
    "\n",
    "xgb_params = {'n_estimators':[100,200,500,1000],\n",
    "              'subsample':[0.6,0.8,1.0],\n",
    "              'max_depth':[3,5,7,9],\n",
    "              'learning_rate':[0.1,0.01,0.001]}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb_cv_model = GridSearchCV(xgb, xgb_params, cv=10, n_jobs=-1, verbose=2).fit(train_pca,y_train)\n",
    "xgb_cv_model.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
